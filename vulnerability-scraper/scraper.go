package main

import (
	"context"
	"fmt"
	"log"
	"strings"
	"sync"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/chromedp/chromedp"
)

// Vulnerability speichert die von einer einzelnen Seite gescrapten Daten
type Vulnerability struct {
	Title       string
	CNVD_ID     string
	PublishedAt string
	Severity    string
	VendorName  string
	ProductName string
	Description string
	PatchInfo   string
}

// Scraper verwaltet den Scraping-Prozess
type Scraper struct {
	db *Database
	// 我们不再需要 http.Client，因为 chromedp 会处理网络请求
}

// NewScraper erstellt einen neuen Scraper
func NewScraper(db *Database) *Scraper {
	return &Scraper{
		db: db,
	}
}

// Run startet den Haupt-Scraping-Loop
func (s *Scraper) Run(maxPages int) {
	// Concurrency control
	var wg sync.WaitGroup
	// Channel, um Detailseiten-URLs an Worker zu übergeben
	urlChan := make(chan string, 100)

	// Start a few workers to process detail pages concurrently
	numWorkers := 5 // 你可以根据你的机器性能调整
	for i := 0; i < numWorkers; i++ {
		wg.Add(1)
		go s.worker(&wg, urlChan)
	}

	// Iterate through list pages
	for i := 1; i <= maxPages; i++ {
		log.Printf("开始使用 Chromedp 爬取列表页: %d", i)
		detailURLs := s.scrapeListPage(i)
		if len(detailURLs) == 0 {
			log.Println("未在列表页找到任何漏洞, 可能已到达最后一页或网站结构已更改。")
			break
		}
		for _, url := range detailURLs {
			urlChan <- url
		}
		// 列表页之间的抓取间隔可以稍微长一点，因为 chromedp 启动需要时间
		time.Sleep(1 * time.Second)
	}

	close(urlChan)
	wg.Wait()
}

// worker ist ein Goroutine-Worker, der URLs vom Kanal liest und verarbeitet
func (s *Scraper) worker(wg *sync.WaitGroup, urlChan <-chan string) {
	defer wg.Done()
	for url := range urlChan {
		vuln, err := s.scrapeDetailPage(url)
		if err != nil {
			log.Printf("使用 Chromedp 爬取详情页失败 %s: %v", url, err)
			continue
		}
		if vuln == nil || vuln.CNVD_ID == "" {
			log.Printf("从 %s 未能抓取到有效数据", url)
			continue
		}
		if err := s.db.SaveVulnerability(vuln); err != nil {
			log.Printf("保存漏洞失败 %s: %v", vuln.CNVD_ID, err)
		}
		// 在每个详情页请求后稍作停顿
		time.Sleep(500 * time.Millisecond)
	}
}

// scrapeListPage 使用 chromedp 爬取数据库漏洞列表页
func (s *Scraper) scrapeListPage(page int) []string {
	const databaseVulnerabilityTypeID = "30"
	url := fmt.Sprintf("https://www.cnvd.org.cn/flaw/typelist?typeId=%s&page=%d", databaseVulnerabilityTypeID, page)

	// 创建一个允许浏览器可见的执行分配器
	allocatorCtx, cancel := chromedp.NewExecAllocator(context.Background(),
		chromedp.ExecPath(`C:\Program Files (x86)\Google\Chrome\Application\chrome.exe`),
		// 移除 headless 模式，让浏览器窗口显示出来
		chromedp.Flag("headless", false),
		// 添加一些有用的标志
		chromedp.NoSandbox,
		chromedp.UserAgent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"),
	)
	defer cancel()

	// 使用上面创建的分配器来创建 chromedp 上下文
	ctx, cancel := chromedp.NewContext(allocatorCtx, chromedp.WithLogf(log.Printf))
	defer cancel()

	// 设置一个合理的超时
	ctx, cancel = context.WithTimeout(ctx, 40*time.Second)
	defer cancel()

	var htmlContent string
	err := chromedp.Run(ctx,
		// 导航到目标 URL
		chromedp.Navigate(url),
		// 等待关键元素出现，确保页面（包括JS）已加载并渲染完成
		// 这是关键一步，等待漏洞列表的表格出现
		chromedp.WaitVisible(`table.tlist tbody`, chromedp.ByQuery),
		// 获取渲染后的整个 HTML 内容
		chromedp.OuterHTML("html", &htmlContent),
	)

	if err != nil {
		log.Printf("Chromedp 执行失败 (列表页 %s): %v", url, err)
		return nil
	}

	// 使用 goquery 解析由 chromedp 获取的 HTML
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(htmlContent))
	if err != nil {
		log.Printf("解析列表页HTML失败: %v", err)
		return nil
	}

	var urls []string
	doc.Find("table.tlist tbody tr").Each(func(i int, sel *goquery.Selection) {
		href, exists := sel.Find("td:nth-child(1) a").Attr("href")
		if exists {
			fullURL := "https://www.cnvd.org.cn" + href
			urls = append(urls, fullURL)
		}
	})
	log.Printf("在列表页 %d 成功找到 %d 个漏洞链接", page, len(urls))
	return urls
}

// scrapeDetailPage 使用 chromedp 爬取漏洞详情页
func (s *Scraper) scrapeDetailPage(url string) (*Vulnerability, error) {
	// 创建一个允许浏览器可见的执行分配器
	allocatorCtx, cancel := chromedp.NewExecAllocator(context.Background(),
		chromedp.ExecPath(`C:\Program Files (x86)\Google\Chrome\Application\chrome.exe`),
		// 移除 headless 模式，让浏览器窗口显示出来
		chromedp.Flag("headless", false),
		// 添加一些有用的标志
		chromedp.NoSandbox,
		chromedp.UserAgent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"),
	)
	defer cancel()

	// 使用上面创建的分配器来创建 chromedp 上下文
	ctx, cancel := chromedp.NewContext(allocatorCtx, chromedp.WithLogf(log.Printf))
	defer cancel()
	ctx, cancel = context.WithTimeout(ctx, 40*time.Second)
	defer cancel()

	var htmlContent string
	err := chromedp.Run(ctx,
		chromedp.Navigate(url),
		// 等待详情页的关键元素（漏洞标题）出现
		chromedp.WaitVisible(`div.gg_detail h1`, chromedp.ByQuery),
		chromedp.OuterHTML("html", &htmlContent),
	)

	if err != nil {
		return nil, fmt.Errorf("chromedp 执行失败 (详情页 %s): %v", url, err)
	}

	doc, err := goquery.NewDocumentFromReader(strings.NewReader(htmlContent))
	if err != nil {
		return nil, err
	}

	vuln := &Vulnerability{}
	vuln.Title = strings.TrimSpace(doc.Find("div.gg_detail h1").Text())

	// Scrape details from the table
	doc.Find("table.gg_detail tbody tr").Each(func(i int, sel *goquery.Selection) {
		header := strings.TrimSpace(sel.Find("td:nth-child(1)").Text())
		value := strings.TrimSpace(sel.Find("td:nth-child(2)").Text())

		switch header {
		case "CNVD-ID":
			vuln.CNVD_ID = value
		case "发布时间":
			vuln.PublishedAt = value
		case "危害级别":
			vuln.Severity = strings.TrimSpace(sel.Find("td:nth-child(2)").Text())
			// 修正危害级别的提取，它可能不在 span 里，也可能在
			if sev := strings.TrimSpace(sel.Find("td:nth-child(2) span").Text()); sev != "" {
				vuln.Severity = sev
			}
		case "厂商":
			vuln.VendorName = value
		case "受影响产品":
			// 该字段可能有多行，我们需要获取完整的 HTML
			prodHtml, _ := sel.Find("td:nth-child(2)").Html()
			vuln.ProductName = strings.TrimSpace(strings.ReplaceAll(prodHtml, "<br/>", "\n"))
		case "漏洞描述":
			vuln.Description = value
		case "参考链接", "补丁信息": // 网站可能用不同的词
			vuln.PatchInfo = value
		}
	})

	if vuln.CNVD_ID != "" {
		log.Printf("成功爬取漏洞: %s - %s", vuln.CNVD_ID, vuln.Title)
	}

	return vuln, nil
}
